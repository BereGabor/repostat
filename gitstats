#!/usr/bin/env python2
# Copyright (c) 2007-2014 Heikki Hokkanen <hoxu@users.sf.net> & others (see doc/AUTHOR)
# GPLv2 / GPLv3
import datetime
import getopt
import glob
import os
import pickle
import shutil
import sys
import time
import zlib
import calendar
import warnings

from jinja2 import Environment, FileSystemLoader
from tools import GitStatistics
from tools import get_external_execution_time, get_pipe_output
from tools import generate_version_from, fetch_contributors_from

os.environ['LC_ALL'] = 'C'

WEEKDAYS = ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')

time_start = time.time()

# By default, gnuplot is searched from path, but can be overridden with the
# environment variable "GNUPLOT"
gnuplot_executable = os.environ.get('GNUPLOT', failobj='gnuplot')

conf = {
    'max_domains': 10,
    'max_ext_length': 10,
    'style': 'gitstats.css',
    'max_authors': 7,
    'authors_top': 5,
    'commit_begin': '',
    'commit_end': 'HEAD',
    'linear_linestats': 1,
    'project_name': '',
    'processes': 8,
    'start_date': ''
}


def getlogrange(defaultrange='HEAD', end_only=True):
    commit_range = getcommitrange(defaultrange, end_only)
    if len(conf['start_date']) > 0:
        return '--since="%s" "%s"' % (conf['start_date'], commit_range)
    return commit_range


def getcommitrange(defaultrange='HEAD', end_only=False):
    if len(conf['commit_end']) > 0:
        if end_only or len(conf['commit_begin']) == 0:
            return conf['commit_end']
        return '%s..%s' % (conf['commit_begin'], conf['commit_end'])
    return defaultrange


def getkeyssortedbyvalues(a_dict):
    return map(lambda el: el[1], sorted(map(lambda el: (el[1], el[0]), a_dict.items())))


# dict['author'] = { 'commits': 512 } - ...key(dict, 'commits')
def getkeyssortedbyvaluekey(d, key):
    return map(lambda el: el[1], sorted(map(lambda el: (d[el][key], el), d.keys())))


def get_project_version():
    try:
        with open('VERSION', 'r') as f:
            version_string = f.readline()
    except IOError as e:
        warnings.warn('This is not a release version of the project?: %s' % e)
        import pygit2 as git
        head_commit = git.Repository(os.getcwd()).head.peel()
        version_string = generate_version_from(head_commit)

    return version_string


def get_project_contributors():
    try:
        with open('CONTRIBUTORS', 'r') as f:
            contributors_list = f.readlines()
    except IOError as e:
        warnings.warn('This is not a release version of the project?: %s' % e)
        import pygit2 as git
        repo = git.Repository(os.getcwd())
        head_commit = repo.head.peel()
        contributors_list = fetch_contributors_from(repo, head_commit)

    return contributors_list


def get_jinja_version():
    import jinja2 as j2
    return '{} v.{}'.format(j2.__name__, j2.__version__)


GNUPLOT_VERSION_STRING = None


def get_gnuplot_version():
    global GNUPLOT_VERSION_STRING
    if GNUPLOT_VERSION_STRING is None:
        GNUPLOT_VERSION_STRING = get_pipe_output(['%s --version' % gnuplot_executable]).split('\n')[0]
    return GNUPLOT_VERSION_STRING


class DataCollector:
    """Manages data collection from a revision control repository."""

    def __init__(self):
        self.stamp_created = time.time()
        self.cache = {}
        self.total_authors = 0
        self.activity_by_hour_of_day = {}  # hour -> commits
        self.activity_by_day_of_week = {}  # day -> commits
        self.activity_by_month_of_year = {}  # month [1-12] -> commits
        self.activity_by_hour_of_week = {}  # weekday -> hour -> commits
        self.activity_by_hour_of_day_busiest = 0
        self.activity_by_hour_of_week_busiest = 0
        self.activity_by_year_week = {}  # yy_wNN -> commits
        self.activity_by_year_week_peak = 0

        # name -> {commits, first_commit_stamp, last_commit_stamp, last_active_day, active_days,
        #  lines_added,
        #  lines_removed}
        self.authors = {}

        self.total_commits = 0
        self.total_files = 0
        self.authors_by_commits = 0

        # domains
        self.domains = {}  # domain -> commits

        # author of the month
        self.author_of_month = {}  # month -> author -> commits
        self.author_of_year = {}  # year -> author -> commits
        self.commits_by_month = {}  # month -> commits
        self.commits_by_year = {}  # year -> commits
        self.lines_added_by_month = {}  # month -> lines added
        self.lines_added_by_year = {}  # year -> lines added
        self.lines_removed_by_month = {}  # month -> lines removed
        self.lines_removed_by_year = {}  # year -> lines removed
        self.first_commit_stamp = 0
        self.last_commit_stamp = 0
        self.active_days = set()

        # lines
        self.total_lines = 0
        self.total_lines_added = 0
        self.total_lines_removed = 0

        # size
        self.total_size = 0

        # timezone
        self.commits_by_timezone = {}  # timezone -> commits

        # tags
        self.tags = {}

        self.files_by_stamp = {}  # stamp -> files

        # extensions
        self.extensions = {}  # extension -> files, lines

        # line statistics
        self.changes_by_date = {}  # stamp -> { files, ins, del }

    ##
    # This should be the main function to extract data from the repository.
    def collect(self, project_directory):
        self.dir = project_directory
        if not conf['project_name']:
            self.projectname = os.path.basename(os.path.abspath(project_directory))
        else:
            self.projectname = conf['project_name']

    ##
    # Load cacheable data
    def loadCache(self, cachefile):
        if not os.path.exists(cachefile):
            return
        print 'Loading cache...'
        f = open(cachefile, 'rb')
        try:
            self.cache = pickle.loads(zlib.decompress(f.read()))
        except:
            # temporary hack to upgrade non-compressed caches
            f.seek(0)
            self.cache = pickle.load(f)
        f.close()

    ##
    # Produce any additional statistics from the extracted data.
    def refine(self):
        pass

    def getFirstCommitDate(self):
        return datetime.datetime.now()

    def getLastCommitDate(self):
        return datetime.datetime.now()

    def getStampCreated(self):
        return self.stamp_created

    ##
    # Save cacheable data
    def saveCache(self, cachefile):
        print 'Saving cache...'
        tempfile = cachefile + '.tmp'
        f = open(tempfile, 'wb')
        # pickle.dump(self.cache, f)
        data = zlib.compress(pickle.dumps(self.cache))
        f.write(data)
        f.close()
        try:
            os.remove(cachefile)
        except OSError:
            pass
        os.rename(tempfile, cachefile)


class GitDataCollector(DataCollector):
    def collect(self, project_directory):
        DataCollector.collect(self, project_directory)
        repo_statistics = GitStatistics(project_directory)

        self.authors = repo_statistics.authors
        self.total_authors = len(repo_statistics.authors)
        self.tags = repo_statistics.tags
        self.domains = repo_statistics.domains
        self.commits_by_timezone = repo_statistics.timezones
        self.first_commit_stamp = repo_statistics.first_commit_timestamp
        self.last_commit_stamp = repo_statistics.last_commit_timestamp
        self.active_days = repo_statistics.active_days
        self.author_of_month = repo_statistics.author_of_month
        self.author_of_year = repo_statistics.author_of_year
        self.activity_by_hour_of_week = repo_statistics.activity_weekly_hourly
        self.activity_by_hour_of_week_busiest = repo_statistics.max_weekly_hourly_activity
        self.activity_by_day_of_week = repo_statistics.get_weekly_activity()
        self.activity_by_hour_of_day = repo_statistics.get_hourly_activity()
        self.activity_by_hour_of_day_busiest = max(self.activity_by_hour_of_day.values())
        self.activity_by_month_of_year = repo_statistics.activity_monthly
        self.commits_by_month = repo_statistics.monthly_commits_timeline
        self.commits_by_year = repo_statistics.yearly_commits_timeline
        self.activity_by_year_week = repo_statistics.recent_activity_by_week
        self.activity_by_year_week_peak = repo_statistics.recent_activity_peak
        self.changes_by_date = repo_statistics.changes_history
        self.total_lines_added = repo_statistics.total_lines_added
        self.total_lines_removed = repo_statistics.total_lines_removed
        self.total_lines = repo_statistics.total_lines_count
        self.lines_added_by_year = repo_statistics.get_lines_insertions_by_year()
        self.lines_removed_by_year = repo_statistics.get_lines_deletions_by_year()
        self.lines_added_by_month = repo_statistics.get_lines_insertions_by_month()
        self.lines_removed_by_month = repo_statistics.get_lines_deletions_by_month()
        self.changes_by_date_by_author = repo_statistics.author_changes_history

        if 'files_in_tree' not in self.cache:
            self.cache['files_in_tree'] = {}
        revs_cached = []
        revs_to_read = []
        # look up rev in cache and take info from cache if found
        # if not append rev to list of rev to read from repo
        for ts, tree_id in repo_statistics.get_revisions():
            # if cache empty then add time and rev to list of new rev's
            # otherwise try to read needed info from cache
            if tree_id in self.cache['files_in_tree'].keys():
                revs_cached.append((ts, self.cache['files_in_tree'][tree_id]))
            else:
                revs_to_read.append((ts, tree_id))

        # update cache with new revisions and append then to general list
        for ts, rev in revs_to_read:
            diff = repo_statistics.get_files_info(rev)
            count = len(diff)
            self.cache['files_in_tree'][rev] = count
            revs_cached.append((ts, count))

        for (stamp, files) in revs_cached:
            self.files_by_stamp[stamp] = files
        self.total_commits = len(self.files_by_stamp)

        ext_dat = {}
        for p in repo_statistics.get_files_info('HEAD'):
            filename = os.path.basename(p.delta.old_file.path)
            basename_ext = filename.split('.')
            ext = basename_ext[1] if len(basename_ext) == 2 and basename_ext[0] else ''
            if len(ext) > conf['max_ext_length']:
                ext = ''
            if ext not in ext_dat:
                ext_dat[ext] = {'files': 0, 'lines': 0}
            # unclear what first two entries of the tuple mean, for each file they were equal to 0
            _, _, lines_count = p.line_stats
            ext_dat[ext]['lines'] += lines_count
            ext_dat[ext]['files'] += 1
        self.extensions = ext_dat
        self.total_files = sum(v['files'] for k, v in ext_dat.items())
        self.total_size = repo_statistics.get_total_size('HEAD')

    def refine(self):
        # authors
        # name -> {place_by_commits, commits_frac, date_first, date_last, timedelta}
        self.authors_by_commits = getkeyssortedbyvaluekey(self.authors, 'commits')
        self.authors_by_commits.reverse()  # most first
        for i, name in enumerate(self.authors_by_commits):
            self.authors[name]['place_by_commits'] = i + 1

        for name in self.authors.keys():
            a = self.authors[name]
            a['commits_frac'] = (100 * float(a['commits'])) / self.getTotalCommits()
            date_first = datetime.datetime.fromtimestamp(a['first_commit_stamp'])
            date_last = datetime.datetime.fromtimestamp(a['last_commit_stamp'])
            delta = date_last - date_first
            a['date_first'] = date_first.strftime('%Y-%m-%d')
            a['date_last'] = date_last.strftime('%Y-%m-%d')
            a['timedelta'] = delta
            if 'lines_added' not in a: a['lines_added'] = 0
            if 'lines_removed' not in a: a['lines_removed'] = 0

    def getActiveDays(self):
        return self.active_days

    def getActivityByDayOfWeek(self):
        return self.activity_by_day_of_week

    def getActivityByHourOfDay(self):
        return self.activity_by_hour_of_day

    def getAuthorInfo(self, author):
        return self.authors[author]

    def getAuthors(self, limit=None):
        res = getkeyssortedbyvaluekey(self.authors, 'commits')
        res.reverse()
        return res[:limit]

    def getCommitDeltaDays(self):
        return (self.last_commit_stamp / 86400 - self.first_commit_stamp / 86400) + 1

    def getDomainInfo(self, domain):
        return self.domains[domain]

    def getDomains(self):
        return self.domains.keys()

    def getFirstCommitDate(self):
        return datetime.datetime.fromtimestamp(self.first_commit_stamp)

    def getLastCommitDate(self):
        return datetime.datetime.fromtimestamp(self.last_commit_stamp)

    def getTags(self):
        lines = get_pipe_output(['git show-ref --tags', 'cut -d/ -f3'])
        return lines.split('\n')

    def getTagDate(self, tag):
        return self.revToDate('tags/' + tag)

    def getTotalAuthors(self):
        return self.total_authors

    def getTotalCommits(self):
        return self.total_commits

    def getTotalFiles(self):
        return self.total_files

    def getTotalLOC(self):
        return self.total_lines

    def getTotalSize(self):
        return self.total_size

    def revToDate(self, rev):
        stamp = int(get_pipe_output(['git log --pretty=format:%%at "%s" -n 1' % rev]))
        return datetime.datetime.fromtimestamp(stamp).strftime('%Y-%m-%d')


class HTMLReportCreator(object):
    def __init__(self):
        self.repostat_root_dir = os.path.dirname(os.path.abspath(__file__))
        templates_dir = os.path.join(self.repostat_root_dir, 'templates')
        self.j2_env = Environment(loader=FileSystemLoader(templates_dir), trim_blocks=True)

    def create(self, data, path):
        self.data = data
        self.path = path
        self.title = data.projectname

        # copy static files. Looks in the binary directory, ../share/gitstats and /usr/share/gitstats
        secondarypath = os.path.join(self.repostat_root_dir, '..', 'share', 'gitstats')
        basedirs = [self.repostat_root_dir, secondarypath, '/usr/share/gitstats']
        for asset in (conf['style'], 'sortable.js', 'arrow-up.gif', 'arrow-down.gif', 'arrow-none.gif'):
            for base in basedirs:
                src = os.path.join(base, asset)
                if os.path.exists(src):
                    shutil.copyfile(src, os.path.join(path, asset))
                    break
            else:
                print 'Warning: "%s" not found, so not copied (searched: %s)' % (asset, basedirs)

        ###
        # General
        general_html = self.render_general_page(data)
        with open(os.path.join(path, "general.html"), 'w') as f:
            f.write(general_html)

        ###
        # Activity
        activity_html = self.render_activity_page(data)
        with open(os.path.join(path, "activity.html"), 'w') as f:
            f.write(activity_html)

        # Weekly activity
        WEEKS = 32

        # generate weeks to show (previous N weeks from now)
        now = datetime.datetime.now()
        deltaweek = datetime.timedelta(7)
        weeks = []
        stampcur = now
        for i in range(0, WEEKS):
            weeks.insert(0, stampcur.strftime('%Y-%W'))
            stampcur -= deltaweek

        # Hour of Day
        hour_of_day = data.getActivityByHourOfDay()
        with open(os.path.join(path, 'hour_of_day.dat'), 'w') as fg:
            for i in range(0, 24):
                fg.write('%d %d\n' % (i + 1, hour_of_day.get(i, 0)))

        # Day of Week
        day_of_week = data.getActivityByDayOfWeek()
        with open(os.path.join(path, 'day_of_week.dat'), 'w') as fp:
            for d in range(0, 7):
                commits = 0
                if d in day_of_week:
                    commits = day_of_week[d]
                fp.write('%d %s %d\n' % (d + 1, WEEKDAYS[d], commits))

        # Month of Year
        with open(os.path.join(path, 'month_of_year.dat'), 'w') as fp:
            for mm in range(1, 13):
                commits = 0
                if mm in data.activity_by_month_of_year:
                    commits = data.activity_by_month_of_year[mm]
                fp.write('%d %d\n' % (mm, commits))

        # Commits by year/month
        with open(os.path.join(path, 'commits_by_year_month.dat'), 'w') as fg:
            for yymm in sorted(data.commits_by_month.keys()):
                fg.write('%s %s\n' % (yymm, data.commits_by_month[yymm]))

        # Commits by year
        with open(os.path.join(path, 'commits_by_year.dat'), 'w') as fg:
            for yy in sorted(data.commits_by_year.keys()):
                fg.write('%d %d\n' % (yy, data.commits_by_year[yy]))

        ###
        # Authors
        authors_html = self.render_authors_page(data)
        with open(os.path.join(path, "authors.html"), 'w') as f:
            f.write(authors_html)

        # cumulated added lines by
        # author. to save memory,
        # changes_by_date_by_author[stamp][author] is defined
        # only at points where author commits.
        # lines_by_authors allows us to generate all the
        # points in the .dat file.
        lines_by_authors = {}

        # Don't rely on getAuthors to give the same order each
        # time. Be robust and keep the list in a variable.
        commits_by_authors = {}

        others_column_name = 'others'
        authors_to_plot = data.getAuthors(conf['max_authors'])
        with open(os.path.join(path, 'lines_of_code_by_author.dat'), 'w') as fgl, \
                open(os.path.join(path, 'commits_by_author.dat'), 'w') as fgc:
            header_row = '"timestamp" ' + ' '.join('"{0}"'.format(w) for w in authors_to_plot) + ' ' \
                         + others_column_name + '\n'
            fgl.write(header_row)
            fgc.write(header_row)
            for stamp in sorted(data.changes_by_date_by_author.keys()):
                fgl.write('%d' % stamp)
                fgc.write('%d' % stamp)
                for author in authors_to_plot:
                    if author in data.changes_by_date_by_author[stamp].keys():
                        lines_by_authors[author] = data.changes_by_date_by_author[stamp][author]['lines_added']
                        commits_by_authors[author] = data.changes_by_date_by_author[stamp][author]['commits']
                    fgl.write(' %d' % lines_by_authors.get(author, 0))
                    fgc.write(' %d' % commits_by_authors.get(author, 0))
                for author in data.changes_by_date_by_author[stamp].keys():
                    if author not in authors_to_plot:
                        lines_by_authors[others_column_name] = lines_by_authors.get(others_column_name, 0) \
                                                               + data.changes_by_date_by_author[stamp][author][
                                                                   'lines_added']
                        commits_by_authors[others_column_name] = commits_by_authors.get(others_column_name, 0) \
                                                                 + data.changes_by_date_by_author[stamp][author][
                                                                     'commits']
                fgl.write(' %d' % lines_by_authors.get(others_column_name, 0))
                fgc.write(' %d' % commits_by_authors.get(others_column_name, 0))
                fgl.write('\n')
                fgc.write('\n')

        # Domains
        domains_by_commits = getkeyssortedbyvaluekey(data.domains, 'commits')
        domains_by_commits.reverse()
        with open(os.path.join(path, 'domains.dat'), 'w') as fp:
            for i, domain in enumerate(domains_by_commits[:conf['max_domains']]):
                info = data.getDomainInfo(domain)
                fp.write('%s %d %d\n' % (domain, i, info['commits']))

        ###
        # Files
        files_html = self.render_files_page(data)
        with open(os.path.join(path, "files.html"), 'w') as f:
            f.write(files_html)

        # FIXME: check if this is still needed
        # use set to get rid of duplicate/unnecessary entries
        files_by_date = set()
        for stamp in sorted(data.files_by_stamp.keys()):
            files_by_date.add('%s %d' %
                              (datetime.datetime.fromtimestamp(stamp).strftime('%Y-%m-%d'), data.files_by_stamp[stamp]))

        with open(os.path.join(path, 'files_by_date.dat'), 'w') as fg:
            for line in sorted(list(files_by_date)):
                fg.write('%s\n' % line)

        ###
        # Lines
        with open(os.path.join(path, 'lines_of_code.dat'), 'w') as fg:
            for stamp in sorted(data.changes_by_date.keys()):
                fg.write('%d %d\n' % (stamp, data.changes_by_date[stamp]['lines']))

        lines_html = self.render_lines_page(data)
        with open(os.path.join(path, "lines.html"), 'w') as f:
            f.write(lines_html)

        ###
        # tags.html
        tags_html = self.render_tags_page(data)
        with open(os.path.join(path, "tags.html"), 'w') as f:
            f.write(tags_html)

        ###
        # about.html
        about_html = self.render_about_page()
        with open(os.path.join(path, "about.html"), 'w') as f:
            f.write(about_html)

        print 'Generating graphs...'
        self.process_gnuplot_scripts(scripts_path=os.path.join(self.repostat_root_dir, 'gnuplot'),
                                     data_path=path,
                                     output_images_path=path)

    def render_general_page(self, data):
        date_format_str = '%Y-%m-%d %H:%M'
        # TODO: this conversion from old 'data' to new 'project data' should perhaps be removed in future
        project_data = {
            "name": data.projectname,
            "age": data.getCommitDeltaDays(),
            "active_days_count": len(data.getActiveDays()),
            "active_days_ratio": "{0:.2f}".format((100.0 * len(data.getActiveDays()) / data.getCommitDeltaDays())),
            "commits_count": data.getTotalCommits(),
            "commits_per_active_day_count": "{0:.2f}".format(float(data.getTotalCommits()) / len(data.getActiveDays())),
            "commits_per_day_count": "{0:.2f}".format(float(data.getTotalCommits()) / data.getCommitDeltaDays()),
            "authors_count": data.getTotalAuthors(),
            "commits_per_author_count": "{0:.2f}".format(float(data.getTotalCommits()) / data.getTotalAuthors()),
            "files_count": data.getTotalFiles(),
            "total_lines_count": data.getTotalLOC(),
            "added_lines_count": data.total_lines_added,
            "removed_lines_count": data.total_lines_removed,
            "first_commit_date": data.getFirstCommitDate().strftime(date_format_str),
            "last_commit_date": data.getLastCommitDate().strftime(date_format_str)
        }

        generation_data = {
            "datetime": datetime.datetime.today().strftime(date_format_str),
            "duration": "{0:.3f}".format(time.time() - data.getStampCreated())
        }

        # load and render template
        template_rendered = self.j2_env.get_template('general.html').render(
            project=project_data,
            generation=generation_data,
            page_title="General"
        )
        return template_rendered

    def render_activity_page(self, data):
        # TODO: this conversion from old 'data' to new 'project data' should perhaps be removed in future
        activity_period_weeks = 32
        project_data = {
            'recent_activity': [],
            'hourly_activity': [],
            'weekday_hourly_activity': {},
            'weekday_activity': {},
            'timezones_activity': []
        }

        # generate weeks to show (previous N weeks from now)
        now = datetime.datetime.now()
        deltaweek = datetime.timedelta(7)
        weeks = []
        stampcur = now
        for i in range(0, activity_period_weeks):
            weeks.insert(0, stampcur.strftime('%Y-%W'))
            stampcur -= deltaweek

        # top row: commits & bar
        for i in range(0, activity_period_weeks):
            commits = 0
            if weeks[i] in data.activity_by_year_week:
                commits = data.activity_by_year_week[weeks[i]]

            percentage = 0
            if weeks[i] in data.activity_by_year_week:
                percentage = float(data.activity_by_year_week[weeks[i]]) / data.activity_by_year_week_peak
            bar_height = max(1, int(200 * percentage))
            project_data['recent_activity'].append((commits, bar_height))

        totalcommits = data.getTotalCommits()
        hour_of_day = data.getActivityByHourOfDay()
        for i in range(0, 24):
            if i in hour_of_day:
                r = 127 + int((float(hour_of_day[i]) / data.activity_by_hour_of_day_busiest) * 128)
                project_data['hourly_activity'].append({'intensity': r, 'commits_count': hour_of_day[i],
                                                        'relative_commits_count': (100.0 * hour_of_day[
                                                            i]) / totalcommits})
            else:
                project_data['hourly_activity'].append(
                    {'intensity': 0, 'commits_count': 0, 'relative_commits_count': 0.00})

        for weekday, weekday_name in enumerate(calendar.day_name):
            project_data['weekday_hourly_activity'][weekday_name] = []
            weekday_commits = 0
            for hour in range(0, 24):
                try:
                    commits = data.activity_by_hour_of_week[weekday][hour]
                    weekday_commits += commits
                except KeyError:
                    commits = 0
                if commits != 0:
                    r = 127 + int((float(commits) / data.activity_by_hour_of_week_busiest) * 128)
                    project_data['weekday_hourly_activity'][weekday_name].append((r, commits))
                else:
                    project_data['weekday_hourly_activity'][weekday_name].append((0, commits))
            project_data['weekday_activity'][weekday_name] = weekday_commits
        total_commits_by_busiest_weekday = max(cc for cc in project_data['weekday_activity'].values())
        for weekday, commit_count in project_data['weekday_activity'].items():
            r = 127 + int((float(commit_count) / total_commits_by_busiest_weekday) * 128)
            project_data['weekday_activity'][weekday] = (r, commit_count)

        max_commits_on_tz = max(data.commits_by_timezone.values())
        for i in sorted(data.commits_by_timezone.keys(), key=lambda n: int(n)):
            commits = data.commits_by_timezone[i]
            r = 127 + int((float(commits) / max_commits_on_tz) * 128)
            project_data['timezones_activity'].append((i, r, commits))

        # load and render template
        template_rendered = self.j2_env.get_template('activity.html').render(
            project=project_data,
            page_title="Activity"
        )
        return template_rendered

    def render_authors_page(self, data):
        # TODO: this conversion from old 'data' to new 'project data' should perhaps be removed in future
        project_data = {
            'top_authors': [],
            'non_top_authors': []
        }

        all_authors = data.getAuthors()
        all_authors = [author.decode('utf-8') for author in all_authors]
        if len(all_authors) > conf['max_authors']:
            rest = all_authors[conf['max_authors']:]
            project_data['non_top_authors'] = rest

        project_data['months'] = []
        for yymm in reversed(sorted(data.author_of_month.keys())):
            authordict = data.author_of_month[yymm]
            authors = getkeyssortedbyvalues(authordict)
            authors.reverse()
            commits = data.author_of_month[yymm][authors[0]]
            next = ', '.join(authors[1:conf['authors_top'] + 1])

            month_dict = {
                'date': yymm,
                'top_author': {'name': authors[0].decode('utf-8'), 'commits_count': commits},
                'next_top_authors': next.decode('utf-8'),
                'all_commits_count': data.commits_by_month[yymm]
            }
            project_data['months'].append(month_dict)

        project_data['years'] = []
        for yy in reversed(sorted(data.author_of_year.keys())):
            authordict = data.author_of_year[yy]
            authors = getkeyssortedbyvalues(authordict)
            authors.reverse()
            commits = data.author_of_year[yy][authors[0]]
            next = ', '.join(authors[1:conf['authors_top'] + 1])

            year_dict = {
                'date': yy,
                'top_author': {'name': authors[0].decode('utf-8'), 'commits_count': commits},
                'next_top_authors': next.decode('utf-8'),
                'all_commits_count': data.commits_by_year[yy]
            }
            project_data['years'].append(year_dict)

        for author in all_authors[:conf['max_authors']]:
            info = data.getAuthorInfo(author)
            author_dict = {
                'name': author,
                'commits_count': info['commits'],
                'commits_relative_count': info['commits_frac'],
                'lines_added_count': info['lines_added'],
                'lines_removed_count': info['lines_removed'],
                'first_commit_date': info['date_first'],
                'latest_commit_date': info['date_last'],
                'contributed_days_count': info['timedelta'],
                'active_days_count': len(info['active_days']),
            }

            project_data['top_authors'].append(author_dict)

        # load and render template
        template_rendered = self.j2_env.get_template('authors.html').render(
            project=project_data,
            page_title="Authors"
        )
        return template_rendered.encode('utf-8')

    def render_files_page(self, data):
        # TODO: this conversion from old 'data' to new 'project data' should perhaps be removed in future
        project_data = {
            'files_count': data.getTotalFiles(),
            'size': data.getTotalSize(),
            'average_file_size': "{0:.2f}".format(float(data.getTotalSize()) / data.getTotalFiles()),
            'files': []
        }

        for ext in sorted(data.extensions.keys()):
            files = data.extensions[ext]['files']
            lines = data.extensions[ext]['lines']
            try:
                loc_percentage = (100.0 * lines) / data.getTotalLOC()
            except ZeroDivisionError:
                loc_percentage = 0
            file_type_dict = {"extension": ext,
                              "count": files,
                              "relative_count": "{0:.2f}".format((100.0 * files) / data.getTotalFiles()),
                              "lines_count": lines,
                              "relative_lines_count": "{0:.2f}".format(loc_percentage),
                              "lines_per_file_count": "{0:.1f}".format(lines / files)
                              }
            project_data['files'].append(file_type_dict)

        # load and render template
        template_rendered = self.j2_env.get_template('files.html').render(
            project=project_data,
            page_title="Files"
        )
        return template_rendered

    def render_lines_page(self, data):
        # TODO: this conversion from old 'data' to new 'project data' should perhaps be removed in future
        project_data = {
            "total_lines_count": data.getTotalLOC(),
            "added_lines_count": data.total_lines_added,
            "removed_lines_count": data.total_lines_removed
        }

        # load and render template
        template_rendered = self.j2_env.get_template('lines.html').render(
            project=project_data,
            page_title="Lines"
        )
        return template_rendered

    def render_tags_page(self, data):
        # TODO: this conversion from old 'data' to new 'project data' should perhaps be removed in future
        project_data = {
            'tags_count': len(data.tags),
            'tags': []
        }

        # TODO: fix error occuring when a tag name and project name are the same
        """
        fatal: ambiguous argument 'gitstats': both revision and filename
        Use '--' to separate paths from revisions, like this:
        'git <command> [<revision>...] -- [<file>...]'
        """

        # TODO: refactor the following code
        tags_sorted_by_date_desc = map(lambda el: el[1],
                                       reversed(sorted(map(lambda el: (el[1]['date'], el[0]), data.tags.items()))))
        for tag in tags_sorted_by_date_desc:
            # there are tags containing no commits
            if 'authors' in data.tags[tag].keys():
                authorinfo = []
                authors_by_commits = getkeyssortedbyvalues(data.tags[tag]['authors'])
                for i in reversed(authors_by_commits):
                    authorinfo.append('%s (%d)' % (i, data.tags[tag]['authors'][i]))
                tag_dict = {
                    'name': tag,
                    'date': data.tags[tag]['date'],
                    'commits_count': data.tags[tag]['commits'],
                    'authors': ', '.join(authorinfo)
                }
                project_data['tags'].append(tag_dict)

        # load and render template
        template_rendered = self.j2_env.get_template('tags.html').render(
            project=project_data,
            page_title="Tags"
        )
        return template_rendered.encode('utf-8')

    def render_about_page(self):
        page_data = {
            "url": "https://github.com/vifactor/repostat",
            "version": get_project_version(),
            "tools": [GitStatistics.get_fetching_tool_info(),
                      get_jinja_version(),
                      get_gnuplot_version()],
            # fixme: contributors evaluation is wrong for any other repo except repostat's one
            # todo: introduce a way to fetch repostat's authors and save them in repostat's codebase
            "contributors": [author.decode('utf-8') for author in get_project_contributors()]
        }

        template_rendered = self.j2_env.get_template('about.html').render(
            repostat=page_data,
            page_title="About"
        )
        return template_rendered.encode('utf-8')

    @staticmethod
    def process_gnuplot_scripts(scripts_path, data_path, output_images_path):
        scripts = glob.glob(os.path.join(scripts_path, '*.plot'))
        os.chdir(output_images_path)
        for script in scripts:
            gnuplot_command = '%s -e "data_folder=\'%s\'" "%s"' % (gnuplot_executable, data_path, script)
            out = get_pipe_output([gnuplot_command])
            if len(out) > 0:
                print out


def usage():
    print """
Usage: gitstats [options] <gitpath..> <outputpath>

Options:
-c key=value     Override configuration value

Default config values:
%s

Please see the manual page for more details.
""" % conf


class GitStats:
    def run(self, args_orig):
        if (3, 0) <= sys.version_info < (2, 6):
            warnings.warn("Python 2.6+ (but <3.0) is required for repostat")
            sys.exit(1)

        if not get_gnuplot_version():
            warnings.warn('gnuplot not found')
            sys.exit(1)

        optlist, args = getopt.getopt(args_orig, 'hc:', ["help"])
        for o, v in optlist:
            if o == '-c':
                key, value = v.split('=', 1)
                if key not in conf:
                    raise KeyError('no such key "%s" in config' % key)
                if isinstance(conf[key], int):
                    conf[key] = int(value)
                else:
                    conf[key] = value
            elif o in ('-h', '--help'):
                usage()
                sys.exit()

        if len(args) < 2:
            usage()
            sys.exit(0)

        outputpath = os.path.abspath(args[-1])
        rundir = os.getcwd()

        try:
            os.makedirs(outputpath)
        except OSError:
            pass
        if not os.path.isdir(outputpath):
            warnings.warn('FATAL: Output path is not a directory or does not exist: %s' % outputpath)
            sys.exit(1)

        print 'Output path: %s' % outputpath
        cachefile = os.path.join(outputpath, 'gitstats.cache')

        data = GitDataCollector()
        data.loadCache(cachefile)

        for gitpath in args[0:-1]:
            print 'Git path: %s' % gitpath

            prevdir = os.getcwd()
            os.chdir(gitpath)

            print 'Collecting data...'
            data.collect(gitpath)

            os.chdir(prevdir)

        print 'Refining data...'
        # data.saveCache(cachefile)
        data.refine()

        os.chdir(rundir)
        print 'Generating report...'
        report = HTMLReportCreator()
        report.create(data, outputpath)

        time_end = time.time()
        exectime_internal = time_end - time_start
        exectime_external = get_external_execution_time()
        print 'Execution time %.5f secs, %.5f secs (%.2f %%) in external commands)' \
              % (exectime_internal, exectime_external, (100.0 * exectime_external) / exectime_internal)
        if sys.stdin.isatty():
            print 'You may now run:'
            print
            print '   sensible-browser \'%s\'' % os.path.join(outputpath, 'general.html').replace("'", "'\\''")
            print


if __name__ == '__main__':
    g = GitStats()
    g.run(sys.argv[1:])
